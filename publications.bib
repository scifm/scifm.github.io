@article{barweyMultiscaleGraphNeural2023,
  title = {Multiscale Graph Neural Network Autoencoders for Interpretable Scientific Machine Learning},
  author = {Barwey, Shivam and Shankar, Varun and Viswanathan, Venkatasubramanian and Maulik, Romit},
  year = 2023,
  month = dec,
  journal = {Journal of Computational Physics},
  volume = {495},
  pages = {112537},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2023.112537},
  url = {https://www.sciencedirect.com/science/article/pii/S0021999123006320},
  urldate = {2025-10-25},
  abstract = {The goal of this work is to address two limitations in autoencoder-based models: latent space interpretability and compatibility with unstructured meshes. This is accomplished here with the development of a novel graph neural network (GNN) autoencoding architecture with demonstrations on complex fluid flow applications. To address the first goal of interpretability, the GNN autoencoder achieves reduction in the number nodes in the encoding stage through an adaptive graph reduction procedure. This reduction procedure essentially amounts to flowfield-conditioned node sampling and sensor identification, and produces interpretable latent graph representations tailored to the flowfield reconstruction task in the form of so-called masked fields. These masked fields allow the user to (a) visualize where in physical space a given latent graph is active, and (b) interpret the time-evolution of the latent graph connectivity in accordance with the time-evolution of unsteady flow features (e.g. recirculation zones, shear layers) in the domain. To address the goal of unstructured mesh compatibility, the autoencoding architecture utilizes a series of multi-scale message passing (MMP) layers, each of which models information exchange among node neighborhoods at various lengthscales. The MMP layer, which augments standard single-scale message passing with learnable coarsening operations, allows the decoder to more efficiently reconstruct the flowfield from the identified regions in the masked fields. Analysis of latent graphs produced by the autoencoder for various model settings are conducted using unstructured snapshot data sourced from large-eddy simulations in a backward-facing step (BFS) flow configuration with an OpenFOAM-based flow solver at high Reynolds numbers.},
  keywords = {Computational fluid dynamics,Graph neural networks,Interpretable machine learning,Scientific machine learning},
  file = {/Users/anoushka/Zotero/storage/3SZQK5G3/Barwey et al. - 2023 - Multiscale graph neural network autoencoders for interpretable scientific machine learning.pdf}
}

@misc{caiExtrapolationAssociationLength2025,
  title = {Extrapolation by {{Association}}: {{Length Generalization Transfer}} in {{Transformers}}},
  shorttitle = {Extrapolation by {{Association}}},
  author = {Cai, Ziyang and Lee, Nayoung and Schwarzschild, Avi and Oymak, Samet and Papailiopoulos, Dimitris},
  year = 2025,
  month = aug,
  number = {arXiv:2506.09251},
  eprint = {2506.09251},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.09251},
  url = {http://arxiv.org/abs/2506.09251},
  urldate = {2025-10-25},
  abstract = {Transformer language models have demonstrated impressive generalization capabilities in natural language domains, yet we lack a fine-grained understanding of how such generalization arises. In this paper, we investigate length generalization--the ability to extrapolate from shorter to longer inputs--through the lens of {\textbackslash}textit\{task association\}. We find that length generalization can be {\textbackslash}textit\{transferred\} across related tasks. That is, training a model with a longer and related auxiliary task can lead it to generalize to unseen and longer inputs from some other target task. We demonstrate this length generalization transfer across diverse algorithmic tasks, including arithmetic operations, string transformations, and maze navigation. Our results show that transformer models can inherit generalization capabilities from similar tasks when trained jointly. Moreover, we observe similar transfer effects in pretrained language models, suggesting that pretraining equips models with reusable computational scaffolding that facilitates extrapolation in downstream settings. Finally, we provide initial mechanistic evidence that length generalization transfer correlates with the re-use of the same attention heads between the tasks. Together, our findings deepen our understanding of how transformers generalize to out-of-distribution inputs and highlight the compositional reuse of inductive structure across tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/anoushka/Zotero/storage/L93YX849/Cai et al. - 2025 - Extrapolation by Association Length Generalization Transfer in Transformers.pdf;/Users/anoushka/Zotero/storage/JYIVEGSL/2506.html}
}

@inproceedings{changAdMiTAdaptiveMultiSource2025,
  title = {{{AdMiT}}: {{Adaptive Multi-Source Tuning}} in {{Dynamic Environments}}},
  shorttitle = {{{AdMiT}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Chang, Xiangyu and Niloy, Fahim Faisal and Ahmed, Sk Miraj and Krishnamurthy, Srikanth V. and Guler, Basak and Swami, Ananthram and Oymak, Samet and Roy-Chowdhury, Amit},
  year = 2025,
  pages = {20569--20579},
  url = {https://openaccess.thecvf.com/content/CVPR2025/html/Chang_AdMiT_Adaptive_Multi-Source_Tuning_in_Dynamic_Environments_CVPR_2025_paper.html},
  urldate = {2025-10-25},
  langid = {english},
  file = {/Users/anoushka/Zotero/storage/NXNBF2NV/Chang et al. - 2025 - AdMiT Adaptive Multi-Source Tuning in Dynamic Environments.pdf}
}

@misc{changFLASHFederatedLearning2025,
  title = {{{FLASH}}: {{Federated Learning Across Simultaneous Heterogeneities}}},
  shorttitle = {{{FLASH}}},
  author = {Chang, Xiangyu and Ahmed, Sk Miraj and Krishnamurthy, Srikanth V. and Guler, Basak and Swami, Ananthram and Oymak, Samet and Roy-Chowdhury, Amit K.},
  year = 2025,
  month = aug,
  number = {arXiv:2402.08769},
  eprint = {2402.08769},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.08769},
  url = {http://arxiv.org/abs/2402.08769},
  urldate = {2025-10-25},
  abstract = {The key premise of federated learning (FL) is to train ML models across a diverse set of data-owners (clients), without exchanging local data. An overarching challenge to this date is client heterogeneity, which may arise not only from variations in data distribution, but also in data quality, as well as compute/communication latency. An integrated view of these diverse and concurrent sources of heterogeneity is critical; for instance, low-latency clients may have poor data quality, and vice versa. In this work, we propose FLASH(Federated Learning Across Simultaneous Heterogeneities), a lightweight and flexible client selection algorithm that outperforms state-of-the-art FL frameworks under extensive sources of heterogeneity, by trading-off the statistical information associated with the client's data quality, data distribution, and latency. FLASH is the first method, to our knowledge, for handling all these heterogeneities in a unified manner. To do so, FLASH models the learning dynamics through contextual multi-armed bandits (CMAB) and dynamically selects the most promising clients. Through extensive experiments, we demonstrate that FLASH achieves substantial and consistent improvements over state-of-the-art baselines -- as much as 10\% in absolute accuracy -- thanks to its unified approach. Importantly, FLASH also outperforms federated aggregation methods that are designed to handle highly heterogeneous settings and even enjoys a performance boost when integrated with them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
  file = {/Users/anoushka/Zotero/storage/YH588ET8/Chang et al. - 2025 - FLASH Federated Learning Across Simultaneous Heterogeneities.pdf;/Users/anoushka/Zotero/storage/7YFPFGI4/2402.html}
}

@misc{changProvableBenefitsTaskSpecific2025,
  title = {Provable {{Benefits}} of {{Task-Specific Prompts}} for {{In-context Learning}}},
  author = {Chang, Xiangyu and Li, Yingcong and Kara, Muti and Oymak, Samet and Roy-Chowdhury, Amit K.},
  year = 2025,
  month = mar,
  number = {arXiv:2503.02102},
  eprint = {2503.02102},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.02102},
  url = {http://arxiv.org/abs/2503.02102},
  urldate = {2025-10-25},
  abstract = {The in-context learning capabilities of modern language models have motivated a deeper mathematical understanding of sequence models. A line of recent work has shown that linear attention models can emulate projected gradient descent iterations to implicitly learn the task vector from the data provided in the context window. In this work, we consider a novel setting where the global task distribution can be partitioned into a union of conditional task distributions. We then examine the use of task-specific prompts and prediction heads for learning the prior information associated with the conditional task distribution using a one-layer attention model. Our results on loss landscape show that task-specific prompts facilitate a covariance-mean decoupling where prompt-tuning explains the conditional mean of the distribution whereas the variance is learned/explained through in-context learning. Incorporating task-specific head further aids this process by entirely decoupling estimation of mean and variance components. This covariance-mean perspective similarly explains how jointly training prompt and attention weights can provably help over fine-tuning after pretraining.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/anoushka/Zotero/storage/R4UI9R9I/Chang et al. - 2025 - Provable Benefits of Task-Specific Prompts for In-context Learning.pdf;/Users/anoushka/Zotero/storage/V4L4SUNW/2503.html}
}

@misc{duraisamyActiveInferenceAI2025,
  title = {Active {{Inference AI Systems}} for {{Scientific Discovery}}},
  author = {Duraisamy, Karthik},
  year = 2025,
  month = aug,
  number = {arXiv:2506.21329},
  eprint = {2506.21329},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.21329},
  url = {http://arxiv.org/abs/2506.21329},
  urldate = {2025-10-25},
  abstract = {The rapid evolution of artificial intelligence has led to expectations of transformative impact on science, yet current systems remain fundamentally limited in enabling genuine scientific discovery. This perspective contends that progress turns on closing three mutually reinforcing gaps in abstraction, reasoning and empirical grounding. Central to addressing these gaps is recognizing complementary cognitive modes: thinking as slow, iterative hypothesis generation -- exploring counterfactual spaces where physical laws can be temporarily violated to discover new patterns -- and reasoning as fast, deterministic validation, traversing established knowledge graphs to test consistency with known principles. Abstractions in this loop should be manipulable models that enable counterfactual prediction, causal attribution, and refinement. Design principles -- rather than a monolithic recipe -- are proposed for systems that reason in imaginary spaces and learn from the world: causal, multimodal models for internal simulation; persistent, uncertainty-aware scientific memory that distinguishes hypotheses from established claims; formal verification pathways coupled to computations and experiments. It is also argued that the inherent ambiguity in feedback from simulations and experiments, and underlying uncertainties make human judgment indispensable, not as a temporary scaffold but as a permanent architectural component. Evaluations must assess the system's ability to identify novel phenomena, propose falsifiable hypotheses, and efficiently guide experimental programs toward genuine discoveries.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Physics - Physics and Society},
  file = {/Users/anoushka/Zotero/storage/JGBYLGF4/Duraisamy - 2025 - Active Inference AI Systems for Scientific Discovery.pdf;/Users/anoushka/Zotero/storage/ZGHQG9G8/2506.html}
}

@misc{gozetenContinuousChainThought2025,
  title = {Continuous {{Chain}} of {{Thought Enables Parallel Exploration}} and {{Reasoning}}},
  author = {Gozeten, Halil Alperen and Ildiz, M. Emrullah and Zhang, Xuechen and Harutyunyan, Hrayr and Rawat, Ankit Singh and Oymak, Samet},
  year = 2025,
  month = sep,
  number = {arXiv:2505.23648},
  eprint = {2505.23648},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.23648},
  url = {http://arxiv.org/abs/2505.23648},
  urldate = {2025-10-25},
  abstract = {Modern language models generate chain-of-thought traces by autoregressively sampling tokens from a finite vocabulary. While this discrete sampling has achieved remarkable success, conducting chain-of-thought with continuously-valued tokens (CoT2) offers a richer and more expressive alternative. Our work provides new theoretical guarantees and algorithms for CoT2, motivated by logical reasoning tasks that inherently require search capabilities. Theoretically, we establish how CoT2 facilitates the model to track multiple discrete traces in parallel; and quantify the level of achievable parallelism and its benefits for inference efficiency. We also provide a CoT2-based one-layer transformer construction that solves the combinatorial "subset sum problem" given a sufficient embedding dimension. These insights arise from a novel and effective supervision strategy where we match the language model outputs to the empirical token distributions of a set of target traces. Complementing this, we introduce sampling strategies that unlock policy optimization methods for CoT2. Our primary strategy samples and composes \$K\$ discrete tokens at each decoding step to control the level of parallelism. Experiments confirm that (i) the optimal level of parallelism is governed by the embedding dimension, (ii) our continuous supervision strategy can outperform alternative methods, and (iii) policy optimization with CoT2 indeed improves the performance of the model beyond its initial discrete or continuous supervision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/anoushka/Zotero/storage/JMPEGFFU/Gozeten et al. - 2025 - Continuous Chain of Thought Enables Parallel Exploration and Reasoning.pdf;/Users/anoushka/Zotero/storage/QFW2TKKS/2505.html}
}

@misc{gozetenTestTimeTrainingProvably2025,
  title = {Test-{{Time Training Provably Improves Transformers}} as {{In-context Learners}}},
  author = {Gozeten, Halil Alperen and Ildiz, M. Emrullah and Zhang, Xuechen and Soltanolkotabi, Mahdi and Mondelli, Marco and Oymak, Samet},
  year = 2025,
  month = mar,
  number = {arXiv:2503.11842},
  eprint = {2503.11842},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.11842},
  url = {http://arxiv.org/abs/2503.11842},
  urldate = {2025-10-25},
  abstract = {Test-time training (TTT) methods explicitly update the weights of a model to adapt to the specific test instance, and they have found success in a variety of settings, including most recently language modeling and reasoning. To demystify this success, we investigate a gradient-based TTT algorithm for in-context learning, where we train a transformer model on the in-context demonstrations provided in the test prompt. Specifically, we provide a comprehensive theoretical characterization of linear transformers when the update rule is a single gradient step. Our theory (i) delineates the role of alignment between pretraining distribution and target task, (ii) demystifies how TTT can alleviate distribution shift, and (iii) quantifies the sample complexity of TTT including how it can significantly reduce the eventual sample size required for in-context learning. As our empirical contribution, we study the benefits of TTT for TabPFN, a tabular foundation model. In line with our theory, we demonstrate that TTT significantly reduces the required sample size for tabular classification (3 to 5 times fewer) unlocking substantial inference efficiency with a negligible training cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/anoushka/Zotero/storage/3RRGM5J6/Gozeten et al. - 2025 - Test-Time Training Provably Improves Transformers as In-context Learners.pdf;/Users/anoushka/Zotero/storage/7IDWEHLT/2503.html}
}

@misc{limDigitalTwinFramework2025,
  title = {A {{Digital Twin Framework}} for {{Generation-IV Reactors}} with {{Reinforcement Learning-Enabled Health-Aware Supervisory Control}}},
  author = {Lim, Jasmin Y. and Pylorof, Dimitrios and Garcia, Humberto E. and Duraisamy, Karthik},
  year = 2025,
  month = jun,
  number = {arXiv:2506.17258},
  eprint = {2506.17258},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.17258},
  url = {http://arxiv.org/abs/2506.17258},
  urldate = {2025-10-25},
  abstract = {Generation IV (Gen-IV) nuclear power plants are envisioned to replace the current reactor fleet, bringing improvements in performance, safety, reliability, and sustainability. However, large cost investments currently inhibit the deployment of these advanced reactor concepts. Digital twins bridge real-world systems with digital tools to reduce costs, enhance decision-making, and boost operational efficiency. In this work, a digital twin framework is designed to operate the Gen-IV Fluoride-salt-cooled High-temperature Reactor, utilizing data-enhanced methods to optimize operational and maintenance policies while adhering to system constraints. The closed-loop framework integrates surrogate modeling, reinforcement learning, and Bayesian inference to streamline end-to-end communication for online regulation and self-adjustment. Reinforcement learning is used to consider component health and degradation to drive the target power generations, with constraints enforced through a Reference Governor control algorithm that ensures compliance with pump flow rate and temperature limits. These input driving modules benefit from detailed online simulations that are assimilated to measurement data with Bayesian filtering. The digital twin is demonstrated in three case studies: a one-year long-term operational period showcasing maintenance planning capabilities, short-term accuracy refinement with high-frequency measurements, and system shock capturing that demonstrates real-time recalibration capabilities when change in boundary conditions. These demonstrations validate robustness for health-aware and constraint-informed nuclear plant operation, with general applicability to other advanced reactor concepts and complex engineering systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/anoushka/Zotero/storage/5ZGAMXSR/Lim et al. - 2025 - A Digital Twin Framework for Generation-IV Reactors with Reinforcement Learning-Enabled Health-Aware.pdf;/Users/anoushka/Zotero/storage/MGTS52Y8/2506.html}
}

@misc{liWhenHowUnlabeled2025,
  title = {When and {{How Unlabeled Data Provably Improve In-Context Learning}}},
  author = {Li, Yingcong and Chang, Xiangyu and Kara, Muti and Liu, Xiaofeng and Roy-Chowdhury, Amit and Oymak, Samet},
  year = 2025,
  month = jun,
  number = {arXiv:2506.15329},
  eprint = {2506.15329},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.15329},
  url = {http://arxiv.org/abs/2506.15329},
  urldate = {2025-10-25},
  abstract = {Recent research shows that in-context learning (ICL) can be effective even when demonstrations have missing or incorrect labels. To shed light on this capability, we examine a canonical setting where the demonstrations are drawn according to a binary Gaussian mixture model (GMM) and a certain fraction of the demonstrations have missing labels. We provide a comprehensive theoretical study to show that: (1) The loss landscape of one-layer linear attention models recover the optimal fully-supervised estimator but completely fail to exploit unlabeled data; (2) In contrast, multilayer or looped transformers can effectively leverage unlabeled data by implicitly constructing estimators of the form \${\textbackslash}sum\_\{i{\textbackslash}ge 0\} a\_i (X{\textasciicircum}{\textbackslash}top X){\textasciicircum}iX{\textasciicircum}{\textbackslash}top y\$ with \$X\$ and \$y\$ denoting features and partially-observed labels (with missing entries set to zero). We characterize the class of polynomials that can be expressed as a function of depth and draw connections to Expectation Maximization, an iterative pseudo-labeling algorithm commonly used in semi-supervised learning. Importantly, the leading polynomial power is exponential in depth, so mild amount of depth/looping suffices. As an application of theory, we propose looping off-the-shelf tabular foundation models to enhance their semi-supervision capabilities. Extensive evaluations on real-world datasets show that our method significantly improves the semisupervised tabular learning performance over the standard single pass inference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/anoushka/Zotero/storage/G4G4YQH2/Li et al. - 2025 - When and How Unlabeled Data Provably Improve In-Context Learning.pdf;/Users/anoushka/Zotero/storage/SAJS5Q99/2506.html}
}

@misc{madhavanSelfsupervisedPretrainingPartial2024a,
  title = {Self-Supervised {{Pretraining}} for {{Partial Differential Equations}}},
  author = {Madhavan, Varun and Sebastian, Amal S. and Ramsundar, Bharath and Viswanathan, Venkatasubramanian},
  year = 2024,
  month = jul,
  number = {arXiv:2407.06209},
  eprint = {2407.06209},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.06209},
  url = {http://arxiv.org/abs/2407.06209},
  urldate = {2025-10-25},
  abstract = {In this work, we describe a novel approach to building a neural PDE solver leveraging recent advances in transformer based neural network architectures. Our model can provide solutions for different values of PDE parameters without any need for retraining the network. The training is carried out in a self-supervised manner, similar to pretraining approaches applied in language and vision tasks. We hypothesize that the model is in effect learning a family of operators (for multiple parameters) mapping the initial condition to the solution of the PDE at any future time step t. We compare this approach with the Fourier Neural Operator (FNO), and demonstrate that it can generalize over the space of PDE parameters, despite having a higher prediction error for individual parameter values compared to the FNO. We show that performance on a specific parameter can be improved by finetuning the model with very small amounts of data. We also demonstrate that the model scales with data as well as model size.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/anoushka/Zotero/storage/RYIA477W/Madhavan et al. - 2024 - Self-supervised Pretraining for Partial Differential Equations.pdf;/Users/anoushka/Zotero/storage/VUZAPEF7/2407.html}
}

@article{sharmaReliableKnowledgeProcessing2024a,
  title = {A Reliable Knowledge Processing Framework for Combustion Science Using Foundation Models},
  author = {Sharma, Vansh and Raman, Venkat},
  year = 2024,
  journal = {Energy and AI},
  volume = {16},
  pages = {100365},
  publisher = {Elsevier},
  url = {https://www.sciencedirect.com/science/article/pii/S2666546824000314},
  urldate = {2025-10-25},
  file = {/Users/anoushka/Zotero/storage/CCJJ3LGC/S2666546824000314.html}
}

@misc{sharmaSteeringConceptualBias2025,
  title = {Steering {{Conceptual Bias}} via {{Transformer Latent-Subspace Activation}}},
  author = {Sharma, Vansh and Raman, Venkat},
  year = 2025,
  month = jun,
  number = {arXiv:2506.18887},
  eprint = {2506.18887},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.18887},
  url = {http://arxiv.org/abs/2506.18887},
  urldate = {2025-10-25},
  abstract = {This work examines whether activating latent subspaces in language models (LLMs) can steer scientific code generation toward a specific programming language. Five causal LLMs were first evaluated on scientific coding prompts to quantify their baseline bias among four programming languages. A static neuron-attribution method, perturbing the highest activated MLP weight for a C++ or CPP token, proved brittle and exhibited limited generalization across prompt styles and model scales. To address these limitations, a gradient-refined adaptive activation steering framework (G-ACT) was developed: per-prompt activation differences are clustered into a small set of steering directions, and lightweight per-layer probes are trained and refined online to select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably biases generation towards the CPP language by increasing the average probe classification accuracy by 15\% and the early layers (0-6) improving the probe classification accuracy by 61.5\% compared to the standard ACT framework. For LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted injections at key layers still improve language selection. Although per-layer probing introduces a modest inference overhead, it remains practical by steering only a subset of layers and enables reproducible model behavior. These results demonstrate a scalable, interpretable and efficient mechanism for concept-level control for practical agentic systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/anoushka/Zotero/storage/K4VZUCDR/Sharma and Raman - 2025 - Steering Conceptual Bias via Transformer Latent-Subspace Activation.pdf;/Users/anoushka/Zotero/storage/ZR9GT2PN/2506.html}
}

@misc{wadellFoundationModelsDiscovery2025a,
  title = {Foundation {{Models}} for {{Discovery}} and {{Exploration}} in {{Chemical Space}}},
  author = {Wadell, Alexius and Bhutani, Anoushka and Azumah, Victor and Ellis-Mohr, Austin R. and Kelly, Celia and Zhao, Hancheng and Nayak, Anuj K. and Hegazy, Kareem and Brace, Alexander and Lin, Hongyi and Emani, Murali and Vishwanath, Venkatram and Gering, Kevin and Alkan, Melisa and Gibbs, Tom and Wells, Jack and Varshney, Lav R. and Ramsundar, Bharath and Duraisamy, Karthik and Mahoney, Michael W. and Ramanathan, Arvind and Viswanathan, Venkatasubramanian},
  year = 2025,
  month = oct,
  number = {arXiv:2510.18900},
  eprint = {2510.18900},
  primaryclass = {physics},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.18900},
  url = {http://arxiv.org/abs/2510.18900},
  urldate = {2025-10-25},
  abstract = {Accurate prediction of atomistic, thermodynamic, and kinetic properties from molecular structures underpins materials innovation. Existing computational and experimental approaches lack the scalability required to efficiently navigate chemical space. Scientific foundation models trained on large unlabeled datasets offer a path toward exploring chemical space across diverse application domains. Here we develop MIST, a family of molecular foundation models with up to an order of magnitude more parameters and data than prior works. Trained using a novel tokenization scheme that comprehensively captures nuclear, electronic, and geometric information, MIST learns from a diverse range of molecules. MIST models have been fine-tuned to predict more than 400 structure -- property relationships and match or exceed state-of-the-art performance across benchmarks spanning physiology, electrochemistry, and quantum chemistry. We demonstrate the ability of these models to solve real-world problems across chemical space, including multiobjective electrolyte solvent screening, olfactory perception mapping, isotope half-life prediction, stereochemical reasoning for chiral organometallic compounds, and binary and multi-component mixture property prediction. Probing MIST models using mechanistic interpretability methods reveals identifiable patterns and trends not explicitly present in the training data, suggesting that the models learn generalizable scientific concepts. We formulate hyperparameter-penalized Bayesian neural scaling laws and use them to reduce the computational cost of model development by an order of magnitude. The methods and findings presented here represent a significant step toward accelerating materials discovery, design, and optimization using foundation models and provide valuable guidance for training compute-optimal scientific foundation models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Materials Science,Physics - Chemical Physics},
  file = {/Users/anoushka/Zotero/storage/TK4IDSPP/Wadell et al. - 2025 - Foundation Models for Discovery and Exploration in Chemical Space.pdf;/Users/anoushka/Zotero/storage/CILSQBEF/2510.html}
}

@inproceedings{wadellScalingFoundationModels2025a,
  title = {Scaling {{Foundation Models}} for {{Molecular Chemistry}}},
  booktitle = {{{AI4X}} 2025 {{International Conference}}},
  author = {Wadell, Alexius and Bhutani, Anoushka and Viswanathan, Venkatasubramanian},
  year = 2025,
  month = apr,
  url = {https://openreview.net/forum?id=PFOmiOoRFP},
  urldate = {2025-10-25},
  langid = {english},
  file = {/Users/anoushka/Zotero/storage/5CMXLCTP/Wadell et al. - 2025 - Scaling Foundation Models for Molecular Chemistry.pdf}
}

@misc{wadellTokenizationMolecularFoundation2025,
  title = {Tokenization for {{Molecular Foundation Models}}},
  author = {Wadell, Alexius and Bhutani, Anoushka and Viswanathan, Venkatasubramanian},
  year = 2025,
  month = jul,
  number = {arXiv:2409.15370},
  eprint = {2409.15370},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.15370},
  url = {http://arxiv.org/abs/2409.15370},
  urldate = {2025-10-25},
  abstract = {Text-based foundation models have become an important part of scientific discovery, with molecular foundation models accelerating advancements in material science and molecular design.However, existing models are constrained by closed-vocabulary tokenizers that capture only a fraction of molecular space. In this work, we systematically evaluate 34 tokenizers, including 19 chemistry-specific ones, and reveal significant gaps in their coverage of the SMILES molecular representation. To assess the impact of tokenizer choice, we introduce n-gram language models as a low-cost proxy and validate their effectiveness by pretraining and finetuning 18 RoBERTa-style encoders for molecular property prediction. To overcome the limitations of existing tokenizers, we propose two new tokenizers -- Smirk and Smirk-GPE -- with full coverage of the OpenSMILES specification. The proposed tokenizers systematically integrate nuclear, electronic, and geometric degrees of freedom; facilitating applications in pharmacology, agriculture, biology, and energy storage. Our results highlight the need for open-vocabulary modeling and chemically diverse benchmarks in cheminformatics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Physics - Chemical Physics,Quantitative Biology - Biomolecules},
  file = {/Users/anoushka/Zotero/storage/FT434FYY/Wadell et al. - 2025 - Tokenization for Molecular Foundation Models.pdf;/Users/anoushka/Zotero/storage/85VHFKP7/2409.html}
}

@misc{wangDREAMSDensityFunctional2025,
  title = {{{DREAMS}}: {{Density Functional Theory Based Research Engine}} for {{Agentic Materials Simulation}}},
  shorttitle = {{{DREAMS}}},
  author = {Wang, Ziqi and Huang, Hongshuo and Zhao, Hancheng and Xu, Changwen and Zhu, Shang and Janssen, Jan and Viswanathan, Venkatasubramanian},
  year = 2025,
  month = jul,
  number = {arXiv:2507.14267},
  eprint = {2507.14267},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.14267},
  url = {http://arxiv.org/abs/2507.14267},
  urldate = {2025-10-25},
  abstract = {Materials discovery relies on high-throughput, high-fidelity simulation techniques such as Density Functional Theory (DFT), which require years of training, extensive parameter fine-tuning and systematic error handling. To address these challenges, we introduce the DFT-based Research Engine for Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for DFT simulation that combines a central Large Language Model (LLM) planner agent with domain-specific LLM agents for atomistic structure generation, systematic DFT convergence testing, High-Performance Computing (HPC) scheduling, and error handling. In addition, a shared canvas helps the LLM agents to structure their discussions, preserve context and prevent hallucination. We validate DREAMS capabilities on the Sol27LC lattice-constant benchmark, achieving average errors below 1{\textbackslash}\% compared to the results of human DFT experts. Furthermore, we apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating its long-term and complex problem-solving capabilities. The framework again reproduces expert-level literature adsorption-energy differences. Finally, DREAMS is employed to quantify functional-driven uncertainties with Bayesian ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS approaches L3-level automation - autonomous exploration of a defined design space - and significantly reduces the reliance on human expertise and intervention, offering a scalable path toward democratized, high-throughput, high-fidelity computational materials discovery.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Condensed Matter - Materials Science},
  file = {/Users/anoushka/Zotero/storage/7BHW3DFM/Wang et al. - 2025 - DREAMS Density Functional Theory Based Research Engine for Agentic Materials Simulation.pdf;/Users/anoushka/Zotero/storage/JNNLDQPP/2507.html}
}

@misc{wuAttentionTrainedEmbeddings2025,
  title = {Attention with {{Trained Embeddings Provably Selects Important Tokens}}},
  author = {Wu, Diyuan and Shevchenko, Aleksandr and Oymak, Samet and Mondelli, Marco},
  year = 2025,
  month = jun,
  number = {arXiv:2505.17282},
  eprint = {2505.17282},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.17282},
  url = {http://arxiv.org/abs/2505.17282},
  urldate = {2025-10-25},
  abstract = {Token embeddings play a crucial role in language modeling but, despite this practical relevance, their theoretical understanding remains limited. Our paper addresses the gap by characterizing the structure of embeddings obtained via gradient descent. Specifically, we consider a one-layer softmax attention model with a linear head for binary classification, i.e., \${\textbackslash}texttt\{Softmax\}( p{\textasciicircum}{\textbackslash}top E\_X{\textasciicircum}{\textbackslash}top ) E\_X v = {\textbackslash}frac\{ {\textbackslash}sum\_\{i=1\}{\textasciicircum}T {\textbackslash}exp(p{\textasciicircum}{\textbackslash}top E\_\{x\_i\}) E\_\{x\_i\}{\textasciicircum}{\textbackslash}top v\}\{{\textbackslash}sum\_\{j=1\}{\textasciicircum}T {\textbackslash}exp(p{\textasciicircum}{\textbackslash}top E\_\{x\_\{j\}\}) \}\$, where \$E\_X = [ E\_\{x\_1\} , {\textbackslash}dots, E\_\{x\_T\} ]{\textasciicircum}{\textbackslash}top\$ contains the embeddings of the input sequence, \$p\$ is the embedding of the \${\textbackslash}mathrm\{{\textbackslash}langle cls {\textbackslash}rangle\}\$ token and \$v\$ the output vector. First, we show that, already after a single step of gradient training with the logistic loss, the embeddings \$E\_X\$ capture the importance of tokens in the dataset by aligning with the output vector \$v\$ proportionally to the frequency with which the corresponding tokens appear in the dataset. Then, after training \$p\$ via gradient flow until convergence, the softmax selects the important tokens in the sentence (i.e., those that are predictive of the label), and the resulting \${\textbackslash}mathrm\{{\textbackslash}langle cls {\textbackslash}rangle\}\$ embedding maximizes the margin for such a selection. Experiments on real-world datasets (IMDB, Yelp) exhibit a phenomenology close to that unveiled by our theory.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/anoushka/Zotero/storage/4FMIIIEL/Wu et al. - 2025 - Attention with Trained Embeddings Provably Selects Important Tokens.pdf;/Users/anoushka/Zotero/storage/A9H9RBHA/2505.html}
}

@misc{xuCLOUDScalablePhysicsInformed2025,
  title = {{{CLOUD}}: {{A Scalable}} and {{Physics-Informed Foundation Model}} for {{Crystal Representation Learning}}},
  shorttitle = {{{CLOUD}}},
  author = {Xu, Changwen and Zhu, Shang and Viswanathan, Venkatasubramanian},
  year = 2025,
  month = jun,
  number = {arXiv:2506.17345},
  eprint = {2506.17345},
  primaryclass = {cond-mat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.17345},
  url = {http://arxiv.org/abs/2506.17345},
  urldate = {2025-10-25},
  abstract = {The prediction of crystal properties is essential for understanding structure-property relationships and accelerating the discovery of functional materials. However, conventional approaches relying on experimental measurements or density functional theory (DFT) calculations are often resource-intensive, limiting their scalability. Machine learning (ML) models offer a promising alternative by learning complex structure-property relationships from data, enabling faster predictions. Yet, existing ML models often rely on labeled data, adopt representations that poorly capture essential structural characteristics, and lack integration with physical principles--factors that limit their generalizability and interpretability. Here, we introduce CLOUD (Crystal Language mOdel for Unified and Differentiable materials modeling), a transformer-based framework trained on a novel Symmetry-Consistent Ordered Parameter Encoding (SCOPE) that encodes crystal symmetry, Wyckoff positions, and composition in a compact, coordinate-free string representation. Pre-trained on over six million crystal structures, CLOUD is fine-tuned on multiple downstream tasks and achieves competitive performance in predicting a wide range of material properties, demonstrating strong scaling performance. Furthermore, as proof of concept of differentiable materials modeling, CLOUD is applied to predict the phonon internal energy and heat capacity, which integrates the Debye model to preserve thermodynamic consistency. The CLOUD-DEBYE framework enforces thermodynamic consistency and enables temperature-dependent property prediction without requiring additional data. These results demonstrate the potential of CLOUD as a scalable and physics-informed foundation model for crystalline materials, unifying symmetry-consistent representations with physically grounded learning for property prediction and materials discovery.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Materials Science},
  file = {/Users/anoushka/Zotero/storage/IDF6B8PM/Xu et al. - 2025 - CLOUD A Scalable and Physics-Informed Foundation Model for Crystal Representation Learning.pdf;/Users/anoushka/Zotero/storage/DBAERMFL/2506.html}
}

@misc{zhangBREADBranchedRollouts2025,
  title = {{{BREAD}}: {{Branched Rollouts}} from {{Expert Anchors Bridge SFT}} & {{RL}} for {{Reasoning}}},
  shorttitle = {{{BREAD}}},
  author = {Zhang, Xuechen and Huang, Zijian and Li, Yingcong and Ni, Chenshun and Chen, Jiasi and Oymak, Samet},
  year = 2025,
  month = jun,
  number = {arXiv:2506.17211},
  eprint = {2506.17211},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.17211},
  url = {http://arxiv.org/abs/2506.17211},
  urldate = {2025-10-25},
  abstract = {Small language models (SLMs) struggle to learn complex reasoning behaviors, especially when high-quality traces are scarce or difficult to learn from. The standard training approach combines a supervised fine-tuning (SFT) stage, often to distill capabilities of a larger model, followed by a reinforcement learning (RL)stage such as Group Relative Policy Optimization (GRPO). In this paper, we investigate the fundamental limitations of this SFT + RL paradigm and propose methods to overcome them. Under a suitable theoretical model, we demonstrate that the SFT + RL strategy can fail completely when (1) the expert's traces are too difficult for the small model to express, or (2) the small model's initialization has exponentially small likelihood of success. To address these, we introduce BREAD: a GRPO variant that unifies the SFT and RL stages via partial expert guidance and branched rollouts. When self-generated traces fail, BREAD adaptively inserts short expert prefixes/hints, allowing the small model to complete the rest of the reasoning path, and ensuring that each update includes at least one successful trace. This mechanism both densifies the reward signal and induces a natural learning curriculum. BREAD requires fewer than 40\% of ground-truth traces, consistently outperforming standard GRPO while speeding up the training by about 3 times. Importantly, we demonstrate that BREAD helps the model solve problems that are otherwise unsolvable by the SFT + RL strategy, highlighting how branched rollouts and expert guidance can substantially boost SLM reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/anoushka/Zotero/storage/NVYTWB7L/Zhang et al. - 2025 - BREAD Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning.pdf;/Users/anoushka/Zotero/storage/JRZSRZ6I/2506.html}
}

@misc{zhuangLaDCastLatentDiffusion2025,
  title = {{{LaDCast}}: {{A Latent Diffusion Model}} for {{Medium-Range Ensemble Weather Forecasting}}},
  shorttitle = {{{LaDCast}}},
  author = {Zhuang, Yilin and Duraisamy, Karthik},
  year = 2025,
  month = jun,
  number = {arXiv:2506.09193},
  eprint = {2506.09193},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.09193},
  url = {http://arxiv.org/abs/2506.09193},
  urldate = {2025-10-25},
  abstract = {Accurate probabilistic weather forecasting demands both high accuracy and efficient uncertainty quantification, challenges that overburden both ensemble numerical weather prediction (NWP) and recent machine-learning methods. We introduce LaDCast, the first global latent-diffusion framework for medium-range ensemble forecasting, which generates hourly ensemble forecasts entirely in a learned latent space. An autoencoder compresses high-dimensional ERA5 reanalysis fields into a compact representation, and a transformer-based diffusion model produces sequential latent updates with arbitrary hour initialization. The model incorporates Geometric Rotary Position Embedding (GeoRoPE) to account for the Earth's spherical geometry, a dual-stream attention mechanism for efficient conditioning, and sinusoidal temporal embeddings to capture seasonal patterns. LaDCast achieves deterministic and probabilistic skill close to that of the European Centre for Medium-Range Forecast IFS-ENS, without any explicit perturbations. Notably, LaDCast demonstrates superior performance in tracking rare extreme events such as cyclones, capturing their trajectories more accurately than established models. By operating in latent space, LaDCast reduces storage and compute by orders of magnitude, demonstrating a practical path toward forecasting at kilometer-scale resolution in real time. We open-source our code and models and provide the training and evaluation pipelines at: https://github.com/tonyzyl/ladcast.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/anoushka/Zotero/storage/XIRESVHI/Zhuang and Duraisamy - 2025 - LaDCast A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting.pdf;/Users/anoushka/Zotero/storage/UQY43Y8Z/2506.html}
}

@article{zhuangSpatiallyawareDiffusionModels2025,
  title = {Spatially-Aware Diffusion Models with Cross-Attention for Global Field Reconstruction with Sparse Observations},
  author = {Zhuang, Yilin and Cheng, Sibo and Duraisamy, Karthik},
  year = 2025,
  month = feb,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {435},
  pages = {117623},
  issn = {0045-7825},
  doi = {10.1016/j.cma.2024.117623},
  url = {https://www.sciencedirect.com/science/article/pii/S0045782524008776},
  urldate = {2025-10-25},
  abstract = {Diffusion models have gained attention for their ability to represent complex distributions and incorporate uncertainty, making them ideal for robust predictions in the presence of noisy or incomplete data. In this study, we develop and enhance score-based diffusion models in field reconstruction tasks, where the goal is to estimate complete spatial fields from partial observations. We introduce a condition encoding approach to construct a tractable mapping between observed and unobserved regions using a learnable integration of sparse observations and interpolated fields as an inductive bias. With refined sensing representations and an unraveled temporal dimension, our method can handle arbitrary moving sensors and effectively reconstruct fields. Furthermore, we conduct a comprehensive benchmark of our approach against a deterministic interpolation-based method across various static and time-dependent PDEs. Our study attempts to addresses the gap in strong baselines for evaluating performance across varying sampling hyperparameters, noise levels, and conditioning methods. Our results show that diffusion models with cross-attention and the proposed conditional encoding generally outperform other methods under noisy conditions, although the deterministic method excels with noiseless data. Additionally, both the diffusion models and the deterministic method surpass the numerical approach in accuracy and computational cost for the steady problem. We also demonstrate the ability of the model to capture possible reconstructions and improve the accuracy of fused results in covariance-based correction tasks using ensemble sampling.},
  keywords = {Diffusion model,Generative AI,Global field reconstruction,Inverse problems},
  file = {/Users/anoushka/Zotero/storage/8RMR9BHL/Zhuang et al. - 2025 - Spatially-aware diffusion models with cross-attention for global field reconstruction with sparse ob.pdf;/Users/anoushka/Zotero/storage/NG2CH5ZH/S0045782524008776.html}
}
